{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Prop3D with Language Models (ESM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install prereqs: pytorch and huggingface transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment if you need to install. For PyTorch GPU installation, follow the instructions on https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user tokenizers transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, EsmForTokenClassification, DataCollatorForTokenClassification\n",
    "from Prop3D.ml.datasets.DistributedDomainSequenceDataset import DistributedDomainSequenceDataset\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HS_ENDPOINT\"] = \"http://prop3d-hsds.pods.uvarc.io\"\n",
    "os.environ[\"HS_USERNAME\"] = \"None\"\n",
    "os.environ[\"HS_PASSWORD\"] = \"None\"\n",
    "\n",
    "cath_file = \"/CATH/Prop3D-20.h5\"\n",
    "cath_superfamily = \"1/10/10/10\" #Use / instead of .\n",
    "\n",
    "#Could be charge, hydrophobicity, accessibility, 3 types of secondary structure, etc\n",
    "predict_features = [\"is_sheet\", \"is_helix\", \"Unk_SS\"] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up ESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "model = EsmForTokenClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=len(predict_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "def collate(x):\n",
    "    sequences, labels = zip(*x)\n",
    "    batch = []\n",
    "    for s, l in x:\n",
    "        s = tokenizer(s)\n",
    "        s[\"labels\"] = np.argmax(l, axis=1)\n",
    "        batch.append(s)\n",
    "\n",
    "    batch = data_collator(batch)\n",
    "    batch[\"input_ids\"].to(device)\n",
    "    batch[\"attention_mask\"].to(device)\n",
    "    batch[\"labels\"].to(device)\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Prop3D datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DistributedDomainSequenceDataset(\n",
    "    cath_file, \n",
    "    cath_superfamily, \n",
    "    predict_features=predict_features, \n",
    "    cluster_level=\"S100\")\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=128, \n",
    "    collate_fn=collate,\n",
    "    shuffle=True)\n",
    "dataset_val = DistributedDomainSequenceDataset(\n",
    "    cath_file, \n",
    "    cath_superfamily, \n",
    "    predict_features=predict_features, \n",
    "    cluster_level=\"S100\",\n",
    "    validation=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset_val, \n",
    "    batch_size=128, \n",
    "    collate_fn=collate,\n",
    "    shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(30):\n",
    "    for loader, is_train in [(training_loader, True), (val_loader, False)]:\n",
    "        running_loss = 0\n",
    "        pbar = tqdm(loader)\n",
    "        for batch in pbar: #enumerate(loader):\n",
    "            # Every data instance is an input + label pair\n",
    "            #inputs, labels = data\n",
    "            #labels = labels.to(device)\n",
    "            #inputs = tokenizer(inputs).to(device)\n",
    "            \n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if is_train:\n",
    "                # Make predictions for this batch\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"], \n",
    "                    attention_mask=batch[\"attention_mask\"], \n",
    "                    labels=batch[\"labels\"])\n",
    "        \n",
    "                # Compute the loss and its gradients\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Adjust learning weights\n",
    "                optimizer.step()\n",
    "\n",
    "                name = \"TRAIN\"\n",
    "\n",
    "            else:\n",
    "                # Make predictions for this batch\n",
    "                outputs = model(inputs, labels=labels)\n",
    "        \n",
    "                # Compute the loss and its gradients\n",
    "                loss = outputs.loss\n",
    "\n",
    "                name = \"VALIDATION\"\n",
    "                \n",
    "            pbar.set_description(f\"Epoch {epoch} {name} Loss {loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "nbsphinx": {
   "execute": "never"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
