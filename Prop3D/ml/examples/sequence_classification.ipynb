{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples on how to use Prop3D as training data to a language model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install prereqs: pytorch and huggingface transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment if you need to install. For PyTorch GPU installation, follow the instructions on https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --user torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{sys.executable} -m pip install --user tokenizers transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EsmForTokenClassification\n",
    "import torch\n",
    "from Prop3D.ml.datasets.DistributedDomainSequenceDataset import DistributedDomainSequenceDataset\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cath_file = \"/projects/Prop3D/Prop3D-20.h5\"\n",
    "cath_superfamily = \"1/10/10/10\" #Use / instead of .\n",
    "\n",
    "#Could be charge, hydrophobicity, accessibility, 3 types of secondary structure, etc\n",
    "predict_features = [\"is_sheet\", \"is_helix\", \"Unk_SS\"] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up ESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "model = EsmForTokenClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=len(predict_features))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Prop3D datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_train = DistributedDomainSequenceDataset(cath_file, cath_superfamily, predict_features=predict_features, split_level=\"S100\")\n",
    "training_loader = torch.utils.data.DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "dataset_val = DistributedDomainSequenceDataset(cath_file, cath_superfamily, predict_features=predict_features, split_level=\"S100\", validation=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=128, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(30):\n",
    "    for loader, is_train in [(training_loader, True), (val_loader, False)]:\n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(loader):\n",
    "            # Every data instance is an input + label pair\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            inputs = tokenizer(inputs)\n",
    "\n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if is_train:\n",
    "                # Make predictions for this batch\n",
    "                outputs = model(inputs, labels=labels)\n",
    "        \n",
    "                # Compute the loss and its gradients\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Adjust learning weights\n",
    "                optimizer.step()\n",
    "\n",
    "                name = \"TRAIN\"\n",
    "\n",
    "            else:\n",
    "                # Make predictions for this batch\n",
    "                outputs = model(inputs, labels=labels)\n",
    "        \n",
    "                # Compute the loss and its gradients\n",
    "                loss = outputs.loss\n",
    "\n",
    "                name = \"VALIDATION\"\n",
    "\n",
    "            running_loss += loss\n",
    "            if i%1000==0:\n",
    "                last_loss = running_loss / 1000 # loss per batch\n",
    "                print('  {} batch {} loss: {}'.format(name, i + 1, last_loss))\n",
    "                running_loss = 0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
